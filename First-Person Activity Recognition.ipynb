{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "CSE527_PA4_fall_21.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5UGefIYpYYNF"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac83f6e8711e4b668fcf8c9a7dfbdef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_26115af9c3c641fa82fcf09dac4a7047",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cd090eb18ca54aee9555a740ee9019a1",
              "IPY_MODEL_8107926ed73d430eb4ea20a8d2d580ed",
              "IPY_MODEL_43f7475bacb84546b8733dd453bac31c"
            ]
          }
        },
        "26115af9c3c641fa82fcf09dac4a7047": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd090eb18ca54aee9555a740ee9019a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8c697d47e86a4301a2f4ac95a9c6a93d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29d3aa422aec47318ecfbc097f813222"
          }
        },
        "8107926ed73d430eb4ea20a8d2d580ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7cf49bf4e7f9439da1f56f3e17fa627b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 52147035,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 52147035,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d14ba2a7597e4474b99d44326dcfd02a"
          }
        },
        "43f7475bacb84546b8733dd453bac31c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_263b47f572a2435584d7896413b77a3b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 49.7M/49.7M [00:00&lt;00:00, 103MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c9cf839b5834178b2e129d6573d48d6"
          }
        },
        "8c697d47e86a4301a2f4ac95a9c6a93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29d3aa422aec47318ecfbc097f813222": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7cf49bf4e7f9439da1f56f3e17fa627b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d14ba2a7597e4474b99d44326dcfd02a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "263b47f572a2435584d7896413b77a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c9cf839b5834178b2e129d6573d48d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSfnuObtYYMH"
      },
      "source": [
        "# CSE527 Programming Assignment 4\n",
        "**Due date: 23:59 on November 26th 2021**\n",
        "\n",
        "In this semester, we will use Google Colab for the assignments, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You will need to use your Stony Brook (*.stonybrook.edu) account for coding and Google Drive to save your results.\n",
        "\n",
        "## Google Colab Tutorial\n",
        "---\n",
        "Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n",
        "\n",
        "Settings used for assignments: ***Edit -> Notebook Settings -> Runtime Type (Python 3)***.\n",
        "\n",
        "\n",
        "## Description\n",
        "---\n",
        "You train a deep network from scratch if you have enough data (it's not always obvious whether or not you do), and if you cannot then instead you fine-tune a pre-trained network as in this problem.\n",
        "\n",
        "In Problem 1, you will be finetuning a pretrained resnet and using it to classify JPL interaction video frames. \n",
        "\n",
        "For Problem 2, you are going to use thread pooling/convolution to classify the video files using the similar pretrained network as your baseline model.\n",
        "\n",
        "\n",
        "\n",
        "There are 2 problems in this homework with a total of 120 points including 20 bonus points. Be sure to read **Submission Guidelines** below. They are important. For the problems requiring text descriptions, you might want to add a markdown block for that.\n",
        "\n",
        "## Dataset\n",
        "---\n",
        "Save the dataset(click me) into your working folder in your Google Drive for this homework. <br>\n",
        "Under your root folder, there should be a folder named \"data\" (i.e. XXX/Surname_Givenname_SBUID/data) containing the images.\n",
        "**Do not upload** the data subfolder before submitting on blackboard due to size limit. There should be only one .ipynb file under your root folder Surname_Givenname_SBUID.\n",
        "\n",
        "## Some Tutorials (PyTorch)\n",
        "---\n",
        "- You will be using PyTorch for deep learning toolbox (follow the [link](http://pytorch.org) for installation).\n",
        "- For PyTorch beginners, please read this [tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) before doing your homework.\n",
        "- Feel free to study more tutorials at http://pytorch.org/tutorials/.\n",
        "- Find cool visualization here at http://playground.tensorflow.org.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UA6WFgcYYMI"
      },
      "source": [
        "# import packages here\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import random \n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7T72O-1ks-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faf5993c-16b9-4897-977b-9293d8f5d466"
      },
      "source": [
        "# Mount your google drive where you've saved your assignment folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVtcfGyUCGe1",
        "outputId": "156fdae2-18f0-4f87-e720-a22caaf91b70"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nBHKIxzAYYM2"
      },
      "source": [
        "## Problem 1 Data Preparation and Fine-tuning\n",
        "## First-Person Activity Recognition: What Are They Doing to Me?\n",
        "In this part of the assignment, you will implement an Activity Classifier using JPL dataset. You will use an an ImageNet pre-trained CNN that serves as a feature extractor.\n",
        "## About JPL dataset\n",
        "This first-person dataset contains videos of interactions between humans and the observer. We attached a GoPro2 camera to the head of our humanoid model, and asked human participants to interact with the humanoid by performing activities. In order to emulate the mobility of a real robot, we also placed wheels below the humanoid and made an operator to move the humanoid by pushing it from the behind. Videos were recorded continuously during human activities where each video sequence contains 0 to 3 activities. The videos are in 320*240 resolution with 30 fps.\n",
        "\n",
        "There are 7 different types of activities in the dataset, including :\n",
        "<ol>\n",
        "\n",
        "###4 positive (i.e., friendly) interactions with the observer: \n",
        "\n",
        "<li> 'Shaking hands with the observer', <li> 'hugging the observer', <li> 'petting the observer', and <li> 'waving a hand to the observer' \n",
        "\n",
        "###1 neutral interaction:\n",
        "<li>  the situation where two persons have a conversation about the observer while occasionally pointing it.\n",
        "\n",
        "###2 negative (i.e., hostile) interactions: \n",
        "<li>  'Punching the observer' and <li> 'throwing objects to the observer'\n",
        "</ol>\n",
        "We will thus assign label to each action, for example:\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "  'Shaking hands with the observer': 1, \n",
        "  'hugging the observer': 2, \n",
        "  'petting the observer': 3, \n",
        "  'waving a hand to the observer': 4,\n",
        "  'the situation where two persons have a conversation about the observer while occasionally pointing it': 5,\n",
        "  'Punching the observer': 6,\n",
        "  'throwing objects to the observer': 7\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ulgWN7XKFH"
      },
      "source": [
        "### Problem 1.0\n",
        "## Loading the JPL dataset: 5 points\n",
        "Check the segmented version from [here](https://drive.google.com/file/d/1eivyF3gPbS3ejea-NYebMBzS40xsRrqF/view?usp=sharing). \n",
        "Save the videos into your working folder in your Google Drive.\n",
        "Under your root folder, there should be a folder named \"data\" (i.e. XXX/Surname_Givenname_SBUID/data) containing the jpl_vid directory where you should extract the jpl dataset. Do not upload the data subfolder before submitting on blackboard due to size limit. There should be only one .ipynb file under your root folder Surname_Givenname_SBUID. \n",
        "In the first part of data preparation, we will convert the videos into images. We will only use all frames from each video and store them as .jpg files. The data folder now also consists of two other directories: jpl_vid, jpl_img. **We will delete the jpl_img directory from you data folder before evaluating.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFtgeZDJCZsm",
        "outputId": "f8ef250a-b864-4585-8aa6-18bef8250a9a"
      },
      "source": [
        "%cd '/content/gdrive/MyDrive/ColabNotebooks/CSE527CV/kumar_arun_114708780_pa4'\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ColabNotebooks/CSE527CV/kumar_arun_114708780_pa4\n",
            "alexnet_trained_nov30_130_v2.pkl  alexnet_trained_v1.pkl    modelpa4q1_gpu.pkl\n",
            "alexnet_trained_nov30.pkl         CSE527_PA4_fall_21.ipynb  modelpa4q1.pkl\n",
            "alexnet_trained_nov30_v2.pkl      \u001b[0m\u001b[01;34mdata\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NboNUWHRkU8l",
        "outputId": "5a1804a8-d27c-4230-b63b-40bbef6aeb42"
      },
      "source": [
        "#make directories\n",
        "!mkdir data\n",
        "!mkdir ./data/jpl_vid\n",
        "!mkdir ./data/jpl_img"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘./data/jpl_vid’: File exists\n",
            "mkdir: cannot create directory ‘./data/jpl_img’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDItayiHvWkI"
      },
      "source": [
        "#Unzip the main video file\n",
        "!cp -r jpl_interaction_segmented_iyuv.zip ./data/jpl_vid\n",
        "%cd ./data/jpl_vid\n",
        "!unzip \"jpl_interaction_segmented_iyuv.zip\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyfozDudlEgj"
      },
      "source": [
        "!rm -rf /content/gdrive/MyDrive/ColabNotebooks/CSE527CV/kumar_arun_114708780_pa4/data/jpl_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqRoYxrd-j4V"
      },
      "source": [
        "!mkdir ./data/jpl_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYOL-2V2vtjx"
      },
      "source": [
        "#to save space remove the ... jpl_interaction_segmented_iyuv.zip file\n",
        "!rm jpl_interaction_segmented_iyuv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6VBwPVq2gPF",
        "outputId": "37ae8a24-8a69-4fa6-e78c-294d1df2dd5a"
      },
      "source": [
        "%cd '/content/gdrive/MyDrive/ColabNotebooks/CSE527CV/kumar_arun_114708780_pa4'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ColabNotebooks/CSE527CV/kumar_arun_114708780_pa4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy-rrhaZ12P_"
      },
      "source": [
        "#Add new imports always here\n",
        "import cv2\n",
        "import os\n",
        "import os, sys\n",
        "cwd = os.getcwd()\n",
        "# import packages here\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import random \n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rVjrRr1YKdfn",
        "outputId": "c2c47c94-d4b9-4aa0-8722-3fdba24cd47c"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/ColabNotebooks/CSE527CV/kumar_arun_114708780_pa4'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVtNIWCRXBvT"
      },
      "source": [
        "def get_frames(filename, n_frames= -1):\n",
        "#--------------------------------------------------\n",
        "#       given the filename of a video,  generate all the frames for that video and return it with length of the frame\n",
        "#       Example: if path /data/jpl_img/10_1/ should contain the frames from 10_1.avi\n",
        "\n",
        "#       if n_frames is -1 store all the frames of the video. \n",
        "#       Else we will only use n_frames frames from each video that are equally spaced across the entire video and store them as .jpg files.\n",
        "#       We expect you to use CV2 library to read video frames.\n",
        "#--------------------------------------------------\n",
        "    \n",
        "    print(filename)\n",
        "    frame_extrct = 0\n",
        "    frames = []\n",
        "\n",
        "    vid = cv2.VideoCapture(filename) \n",
        "    frame_count = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    count = 0\n",
        "\n",
        "    if n_frames == -1:\n",
        "      jump = 1\n",
        "    else:\n",
        "      jump = frame_count/n_frames\n",
        "\n",
        "    while vid.isOpened():\n",
        "      ret,frame = vid.read()\n",
        "\n",
        "      if frame_extrct >= n_frames:\n",
        "        break\n",
        "      if ret:\n",
        "        frame_extrct = frame_extrct + 1\n",
        "        frames.append(frame)\n",
        "        count += jump\n",
        "        vid.set(cv2.CAP_PROP_POS_FRAMES, count)\n",
        "      else:\n",
        "        vid.release()\n",
        "        break\n",
        "    \n",
        "    v_len = len(frames)\n",
        "    print(v_len)\n",
        "\n",
        "    return frames, v_len\n",
        "    \n",
        "def store_frames(frames, path2store):\n",
        "    for ii, frame in enumerate(frames):\n",
        "        frame = cv2.cvtColor(np.float32(frame), cv2.COLOR_RGB2BGR)  \n",
        "        path2img = os.path.join(path2store, \"frame\"+str(ii)+\".jpg\")\n",
        "        cv2.imwrite(path2img, frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNC6g66hqfsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ac02f6-8613-402a-b03c-e29d854536c3"
      },
      "source": [
        "path2data = \"./data\"\n",
        "sub_folder = \"jpl_vid\"\n",
        "sub_folder_jpg = \"jpl_img\"\n",
        "path2video = os.path.join(path2data, sub_folder)\n",
        "listOfCategories = os.listdir(path2video)\n",
        "listOfCategories, len(listOfCategories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['1_1.avi',\n",
              "  '1_2.avi',\n",
              "  '1_3.avi',\n",
              "  '1_4.avi',\n",
              "  '1_5.avi',\n",
              "  '1_6.avi',\n",
              "  '1_7.avi',\n",
              "  '2_1.avi',\n",
              "  '2_2.avi',\n",
              "  '2_3.avi',\n",
              "  '2_4.avi',\n",
              "  '2_5.avi',\n",
              "  '2_6.avi',\n",
              "  '2_7.avi',\n",
              "  '3_1.avi',\n",
              "  '3_2.avi',\n",
              "  '3_3.avi',\n",
              "  '3_5.avi',\n",
              "  '3_6.avi',\n",
              "  '3_7.avi',\n",
              "  '4_1.avi',\n",
              "  '4_2.avi',\n",
              "  '4_3.avi',\n",
              "  '4_4_1.avi',\n",
              "  '4_4_2.avi',\n",
              "  '4_5.avi',\n",
              "  '4_6.avi',\n",
              "  '4_7.avi',\n",
              "  '5_1.avi',\n",
              "  '5_2.avi',\n",
              "  '5_3.avi',\n",
              "  '5_4.avi',\n",
              "  '5_5.avi',\n",
              "  '5_6.avi',\n",
              "  '5_7.avi',\n",
              "  '6_1.avi',\n",
              "  '6_2.avi',\n",
              "  '6_3.avi',\n",
              "  '6_4.avi',\n",
              "  '6_5.avi',\n",
              "  '6_7.avi',\n",
              "  '7_1.avi',\n",
              "  '7_2.avi',\n",
              "  '7_3.avi',\n",
              "  '7_4.avi',\n",
              "  '7_5.avi',\n",
              "  '7_7.avi',\n",
              "  '8_1.avi',\n",
              "  '8_2.avi',\n",
              "  '8_3.avi',\n",
              "  '8_5.avi',\n",
              "  '8_7.avi',\n",
              "  '8_6.avi',\n",
              "  '7_6.avi',\n",
              "  '6_6.avi',\n",
              "  '8_4.avi',\n",
              "  '9_7.avi',\n",
              "  '9_6.avi',\n",
              "  '9_4.avi',\n",
              "  '9_1.avi',\n",
              "  '9_3.avi',\n",
              "  '9_5.avi',\n",
              "  '9_2.avi',\n",
              "  '10_7.avi',\n",
              "  '10_6.avi',\n",
              "  '10_4.avi',\n",
              "  '10_1.avi',\n",
              "  '10_2.avi',\n",
              "  '10_5.avi',\n",
              "  '10_3.avi',\n",
              "  '11_7.avi',\n",
              "  '11_6.avi',\n",
              "  '11_4.avi',\n",
              "  '11_1.avi',\n",
              "  '11_3.avi',\n",
              "  '11_5.avi',\n",
              "  '11_2.avi',\n",
              "  '12_7.avi',\n",
              "  '12_6.avi',\n",
              "  '12_4.avi',\n",
              "  '12_1.avi',\n",
              "  '12_3.avi',\n",
              "  '12_5.avi',\n",
              "  '12_2.avi'],\n",
              " 84)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88PWhprw-ASO"
      },
      "source": [
        "n_frames = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyMtynYTqbu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db084c36-a2f1-47c0-c041-af94e5a1ad43"
      },
      "source": [
        "extension = \".avi\"\n",
        "#--------------------------------------------------\n",
        "#choose a value for n_frames below to optimize your solution. We might randomly choose n_frames while evaluating \n",
        "#--------------------------------------------------\n",
        "\n",
        "for root, dirs, files in os.walk(path2video, topdown=False):\n",
        "    for name in files:\n",
        "        if extension not in name:\n",
        "            continue\n",
        "        path2vid = os.path.join(root, name)\n",
        "        frames, vlen = get_frames(path2vid, n_frames= n_frames)\n",
        "        print(vlen)\n",
        "        path2store = path2vid.replace(sub_folder, sub_folder_jpg)\n",
        "        path2store = path2store.replace(extension, \"\")\n",
        "        print(path2store)\n",
        "        os.makedirs(path2store, exist_ok= True)\n",
        "        store_frames(frames, path2store)\n",
        "    print(\"-\"*50) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data/jpl_vid/1_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/1_1\n",
            "./data/jpl_vid/1_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/1_2\n",
            "./data/jpl_vid/1_3.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/1_3\n",
            "./data/jpl_vid/1_4.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/1_4\n",
            "./data/jpl_vid/1_5.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/1_5\n",
            "./data/jpl_vid/1_6.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/1_6\n",
            "./data/jpl_vid/1_7.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/1_7\n",
            "./data/jpl_vid/2_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/2_1\n",
            "./data/jpl_vid/2_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/2_2\n",
            "./data/jpl_vid/2_3.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/2_3\n",
            "./data/jpl_vid/2_4.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/2_4\n",
            "./data/jpl_vid/2_5.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/2_5\n",
            "./data/jpl_vid/2_6.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/2_6\n",
            "./data/jpl_vid/2_7.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/2_7\n",
            "./data/jpl_vid/3_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/3_1\n",
            "./data/jpl_vid/3_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/3_2\n",
            "./data/jpl_vid/3_3.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/3_3\n",
            "./data/jpl_vid/3_5.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/3_5\n",
            "./data/jpl_vid/3_6.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/3_6\n",
            "./data/jpl_vid/3_7.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/3_7\n",
            "./data/jpl_vid/4_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/4_1\n",
            "./data/jpl_vid/4_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/4_2\n",
            "./data/jpl_vid/4_3.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/4_3\n",
            "./data/jpl_vid/4_4_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/4_4_1\n",
            "./data/jpl_vid/4_4_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/4_4_2\n",
            "./data/jpl_vid/4_5.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/4_5\n",
            "./data/jpl_vid/4_6.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/4_6\n",
            "./data/jpl_vid/4_7.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/4_7\n",
            "./data/jpl_vid/5_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/5_1\n",
            "./data/jpl_vid/5_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/5_2\n",
            "./data/jpl_vid/5_3.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/5_3\n",
            "./data/jpl_vid/5_4.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/5_4\n",
            "./data/jpl_vid/5_5.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/5_5\n",
            "./data/jpl_vid/5_6.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/5_6\n",
            "./data/jpl_vid/5_7.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/5_7\n",
            "./data/jpl_vid/6_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/6_1\n",
            "./data/jpl_vid/6_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/6_2\n",
            "./data/jpl_vid/6_3.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/6_3\n",
            "./data/jpl_vid/6_4.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/6_4\n",
            "./data/jpl_vid/6_5.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/6_5\n",
            "./data/jpl_vid/6_7.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/6_7\n",
            "./data/jpl_vid/7_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/7_1\n",
            "./data/jpl_vid/7_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/7_2\n",
            "./data/jpl_vid/7_3.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/7_3\n",
            "./data/jpl_vid/7_4.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/7_4\n",
            "./data/jpl_vid/7_5.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/7_5\n",
            "./data/jpl_vid/7_7.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/7_7\n",
            "./data/jpl_vid/8_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/8_1\n",
            "./data/jpl_vid/8_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/8_2\n",
            "./data/jpl_vid/8_3.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/8_3\n",
            "./data/jpl_vid/8_5.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/8_5\n",
            "./data/jpl_vid/8_7.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/8_7\n",
            "./data/jpl_vid/8_6.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/8_6\n",
            "./data/jpl_vid/7_6.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/7_6\n",
            "./data/jpl_vid/6_6.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/6_6\n",
            "./data/jpl_vid/8_4.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/8_4\n",
            "./data/jpl_vid/9_7.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/9_7\n",
            "./data/jpl_vid/9_6.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/9_6\n",
            "./data/jpl_vid/9_4.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/9_4\n",
            "./data/jpl_vid/9_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/9_1\n",
            "./data/jpl_vid/9_3.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/9_3\n",
            "./data/jpl_vid/9_5.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/9_5\n",
            "./data/jpl_vid/9_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/9_2\n",
            "./data/jpl_vid/10_7.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/10_7\n",
            "./data/jpl_vid/10_6.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/10_6\n",
            "./data/jpl_vid/10_4.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/10_4\n",
            "./data/jpl_vid/10_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/10_1\n",
            "./data/jpl_vid/10_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/10_2\n",
            "./data/jpl_vid/10_5.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/10_5\n",
            "./data/jpl_vid/10_3.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/10_3\n",
            "./data/jpl_vid/11_7.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/11_7\n",
            "./data/jpl_vid/11_6.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/11_6\n",
            "./data/jpl_vid/11_4.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/11_4\n",
            "./data/jpl_vid/11_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/11_1\n",
            "./data/jpl_vid/11_3.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/11_3\n",
            "./data/jpl_vid/11_5.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/11_5\n",
            "./data/jpl_vid/11_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/11_2\n",
            "./data/jpl_vid/12_7.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/12_7\n",
            "./data/jpl_vid/12_6.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/12_6\n",
            "./data/jpl_vid/12_4.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/12_4\n",
            "./data/jpl_vid/12_1.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/12_1\n",
            "./data/jpl_vid/12_3.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/12_3\n",
            "./data/jpl_vid/12_5.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/12_5\n",
            "./data/jpl_vid/12_2.avi\n",
            "30\n",
            "30\n",
            "./data/jpl_img/12_2\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cOZcputzDa0"
      },
      "source": [
        "## Training, Test and Validation set\n",
        "**Training set:**\n",
        "Participant 1-9\n",
        "\n",
        "\n",
        "**Test set:**\n",
        "Participant 10 - 12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmntRcKHH_AO"
      },
      "source": [
        "def prepare_sets(path2ajpgs):\n",
        "    listOfCats = os.listdir(path2ajpgs)\n",
        "    train_id = []\n",
        "    train_label = []\n",
        "    test_id = []\n",
        "    test_label = []\n",
        "    for i in range(len(listOfCats)):\n",
        "      x = listOfCats[i].split(\"_\")\n",
        "      if(int(x[0]) > 9):\n",
        "        test_id.append(listOfCats[i])\n",
        "        test_label.append(int(x[1])-1)\n",
        "      else:\n",
        "        train_id.append(listOfCats[i])\n",
        "        train_label.append(int(x[1])-1)\n",
        "      \n",
        "    return train_id, train_label, test_id, test_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpqKf7A4eG0G"
      },
      "source": [
        "path2jpg = os.path.join(path2data, sub_folder_jpg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXyuLJBlIMKM"
      },
      "source": [
        "train_ids, train_labels, test_ids, test_labels = prepare_sets(path2jpg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS6V7rktbcbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a50846-7aeb-41f1-bc18-df50e9a24c5d"
      },
      "source": [
        "test_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 5, 3, 0, 1, 4, 2, 6, 5, 3, 0, 2, 4, 1, 6, 5, 3, 0, 2, 4, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu1dseEo2Ce8"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "np.random.seed(2020)\n",
        "random.seed(2020)\n",
        "torch.manual_seed(2020)\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, ids, labels,transform):      \n",
        "        self.transform = transform\n",
        "        self.ids = ids\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        path2imgs=glob.glob(path2jpg+\"/\"+self.ids[idx]+\"/*.jpg\")\n",
        "        path2imgs = path2imgs[:n_frames]\n",
        "        label = self.labels[idx]\n",
        "        frames = []\n",
        "        for p2i in path2imgs:\n",
        "            frame = Image.open(p2i)\n",
        "            frames.append(frame)\n",
        "        \n",
        "        seed = np.random.randint(1e9)        \n",
        "        frames_tr = []\n",
        "        for frame in frames:\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "            frame = self.transform(frame) \n",
        "            frames_tr.append(frame)\n",
        "        if len(frames_tr)>0:\n",
        "            frames_tr = torch.stack(frames_tr)\n",
        "        #print(len(frames_tr))\n",
        "        #for xb,yb in frames_tr:\n",
        "        #frames_tr = torch.reshape(frames_tr, ( -1, 240, 320))\n",
        "        return frames_tr, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SviYj6QU05YL"
      },
      "source": [
        "### Problem 1.1\n",
        "## Dataloader: 5 points\n",
        "We now need to create scripts so that it accepts the generator that we just created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeetLEZe5XfU",
        "outputId": "acbeadd8-6cdb-4ee9-d028-8aec6a67ca28"
      },
      "source": [
        "train_ds = VideoDataset(ids= train_ids, labels= train_labels,\n",
        "                                                      transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                                      transforms.Resize((224,224), transforms.InterpolationMode.BILINEAR),\n",
        "                                                      transforms.Normalize(mean = (0.485, 0.456, 0.406),\n",
        "                                                      std = (0.229, 0.224, 0.225))]))\n",
        "\n",
        "print(len(train_ds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeSo9wsuVJQa",
        "outputId": "03adf04e-81ec-40b4-819c-551fc3cdc440"
      },
      "source": [
        "test_ds = VideoDataset(ids= test_ids, labels= test_labels,\n",
        "                                                     transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                                     transforms.Resize((224,224),transforms.InterpolationMode.BILINEAR),\n",
        "                                                     transforms.Normalize(mean = (0.485, 0.456, 0.406), \n",
        "                                                      std = (0.229, 0.224, 0.225))]))\n",
        "print(len(test_ds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo4mpoOBCSmI"
      },
      "source": [
        "#train_combine = ConcatDataset[train_ds_flip, train_ds]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj0D75HAVu8d"
      },
      "source": [
        "#--------------------------------------------------\n",
        "#create dataloader for all the datasets(train and test) \n",
        "#--------------------------------------------------\n",
        "train_dl = DataLoader(train_ds, batch_size=7, shuffle=True, num_workers=1)\n",
        "test_dl  = DataLoader(test_ds , batch_size=7, shuffle=False, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s_8MvxbAXWr"
      },
      "source": [
        "#train_combine = [d for dl in [train_dl, train_dl_flip] for d in dl]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9j0-QJfaITE",
        "outputId": "42c5f0d6-d433-4150-c947-ff4facb8c0f3"
      },
      "source": [
        "print(train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 4, 5, 6, 0, 1, 2, 3, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 6, 0, 1, 2, 3, 4, 6, 0, 1, 2, 4, 6, 5, 5, 5, 3, 6, 5, 3, 0, 2, 4, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni0S_7anc-gP",
        "outputId": "d98e6f04-742f-466c-e607-a09af732f7c8"
      },
      "source": [
        "print(test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 5, 3, 0, 1, 4, 2, 6, 5, 3, 0, 2, 4, 1, 6, 5, 3, 0, 2, 4, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ_Isziq8Pi3"
      },
      "source": [
        "dataloaders = []\n",
        "dataloaders.append(train_dl)\n",
        "dataloaders.append(test_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHYFd0iS8VVu"
      },
      "source": [
        "dataset_sizes = []\n",
        "dataset_sizes.append(63)\n",
        "dataset_sizes.append(21)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er-YBeSIlEGg",
        "outputId": "c40eee36-510f-4331-cb7a-5ae34982b0a2"
      },
      "source": [
        "for xb,yb in train_dl:\n",
        "    print(xb.shape, yb.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7, 30, 3, 224, 224]) torch.Size([7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1vRVJRwhKZe"
      },
      "source": [
        "## Problem 1.2: Fine Tuning a Pre-Trained Deep Network: 40 points\n",
        "The representations learned by deep convolutional networks generalize surprisingly well to other recognition tasks. \n",
        "\n",
        "But how do we use an existing deep network for a new recognition task? Take for instance,  [ResNet network](https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html) [(paper)](https://arxiv.org/abs/1512.03385).\n",
        "\n",
        "\n",
        "**Hints**:\n",
        "- Many pre-trained models are available in PyTorch at [here](http://pytorch.org/docs/master/torchvision/models.html).\n",
        "- For fine-tuning pretrained network using PyTorch, please read this [tutorial](http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKHsertZhq_r"
      },
      "source": [
        "###Problem 1.2.1\n",
        "\n",
        "##*Fine-tune* an existing network: 30 points\n",
        " In this scenario you take an existing network, replace the final layer (or more) with random weights, and train the entire network again with images and ground truth labels for your recognition task. You are effectively treating the pre-trained deep network as a better initialization than the random weights used when training from scratch. When you don't have enough training data to train a complex network from scratch (e.g. with the 7 classes) this is an attractive option. In [this paper](http://www.cc.gatech.edu/~hays/papers/deep_geo.pdf) from CVPR 2015, there wasn't enough data to train a deep network from scratch, but fine tuning led to 4 times higher accuracy than using off-the-shelf networks directly.\n",
        " You are required to implement above strategy to fine-tune a pre-trained **ResNet** for this video frames classification task with 7 classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J7XwL2y58dc"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC5aC-MwmFrb"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtvUtwdsxZrT"
      },
      "source": [
        "#import resnet\n",
        "import torchvision\n",
        "r3d_18_pre = torchvision.models.video.r3d_18(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAxTMdCXhmGZ"
      },
      "source": [
        "#--------------------------------------------------\n",
        "#       Fine-Tune Pretrained Network\n",
        "#--------------------------------------------------\n",
        "r3d_18_pre = r3d_18_pre.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.Adam(r3d_18_pre.parameters(), lr= 0.001)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwSvS7XL-Mmm",
        "outputId": "c135dc2b-29a5-441a-a325-6a51689abf04"
      },
      "source": [
        "r3d_18_pre"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VideoResNet(\n",
              "  (stem): BasicStem(\n",
              "    (0): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
              "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=400, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzKCUeuIrwqz"
      },
      "source": [
        "#for param in r3d_18_pre.parameters():\n",
        "#    param.requires_grad = False\n",
        "\n",
        "r3d_18_pre.stem[0] = nn.Conv3d(n_frames, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
        "r3d_18_pre.fc = nn.Linear(in_features=512, out_features=7, bias=True)\n",
        "\n",
        "# for child in r3d_18_pre.children():\n",
        "#     for param in child.parameters():\n",
        "#         param.requires_grad = False\n",
        "\n",
        "# for layer in list(r3d_18_pre.children())[-4:]:\n",
        "#   print(layer)\n",
        "#   layer.requires_grad = True\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fiE7wsMlVu4"
      },
      "source": [
        "###Problem 1.2.2\n",
        "###Training and Testing your fine-tuned Network: 10 points\n",
        "You will fine-tune your network using every frame in the video as a sample with the class label. Use train_dl and test_dl and feed it to your fine-tuned network. Please provide detailed descriptions of:<br>\n",
        "(1) which layers of Resnet have been replaced<br>\n",
        "(2) the architecture of the new layers added including activation methods <br>\n",
        "(3) the final accuracy on test set <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RytWgBzlrRkc",
        "outputId": "8a4d22a0-28f0-443f-d33c-b24b9bbad7a9"
      },
      "source": [
        "dataset_sizes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[63, 21]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykfTI8YFmxkd"
      },
      "source": [
        "#--------------------------------------------------\n",
        "#       Train your model\n",
        "#--------------------------------------------------\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "    model.to(device)\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in range(len(dataloaders)):\n",
        "            if phase == 0:\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                \n",
        "\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 0):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    #loss.requires_grad = True\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 0:\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "  \n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 0:\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 1 and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrlrd0WOnRXJ",
        "outputId": "0df0f005-a69b-47f1-acf4-0c4033e49251"
      },
      "source": [
        "model_ft = train_model(r3d_18_pre, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "0 Loss: 1.6657 Acc: 0.3492\n",
            "1 Loss: 4.3657 Acc: 0.1905\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "0 Loss: 0.6963 Acc: 0.6984\n",
            "1 Loss: 6.8464 Acc: 0.1429\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "0 Loss: 0.4383 Acc: 0.8889\n",
            "1 Loss: 5.2877 Acc: 0.2381\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "0 Loss: 0.2417 Acc: 0.9048\n",
            "1 Loss: 2.4201 Acc: 0.4286\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "0 Loss: 0.1557 Acc: 0.9524\n",
            "1 Loss: 1.6532 Acc: 0.4286\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "0 Loss: 0.0630 Acc: 1.0000\n",
            "1 Loss: 2.4974 Acc: 0.2857\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "0 Loss: 0.0945 Acc: 0.9841\n",
            "1 Loss: 1.6982 Acc: 0.3810\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "0 Loss: 0.0422 Acc: 1.0000\n",
            "1 Loss: 1.1683 Acc: 0.5714\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "0 Loss: 0.0381 Acc: 1.0000\n",
            "1 Loss: 0.9356 Acc: 0.6190\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "0 Loss: 0.0180 Acc: 1.0000\n",
            "1 Loss: 0.8567 Acc: 0.6667\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "0 Loss: 0.0145 Acc: 1.0000\n",
            "1 Loss: 0.8191 Acc: 0.7143\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "0 Loss: 0.0230 Acc: 1.0000\n",
            "1 Loss: 0.8137 Acc: 0.7143\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "0 Loss: 0.0247 Acc: 1.0000\n",
            "1 Loss: 0.8261 Acc: 0.7143\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "0 Loss: 0.0133 Acc: 1.0000\n",
            "1 Loss: 0.8277 Acc: 0.7143\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "0 Loss: 0.0074 Acc: 1.0000\n",
            "1 Loss: 0.8162 Acc: 0.7143\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "0 Loss: 0.0100 Acc: 1.0000\n",
            "1 Loss: 0.8134 Acc: 0.7143\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "0 Loss: 0.0124 Acc: 1.0000\n",
            "1 Loss: 0.8217 Acc: 0.6667\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "0 Loss: 0.0107 Acc: 1.0000\n",
            "1 Loss: 0.8103 Acc: 0.7143\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n",
            "0 Loss: 0.0101 Acc: 1.0000\n",
            "1 Loss: 0.8183 Acc: 0.6667\n",
            "\n",
            "Epoch 19/24\n",
            "----------\n",
            "0 Loss: 0.0145 Acc: 1.0000\n",
            "1 Loss: 0.8198 Acc: 0.6667\n",
            "\n",
            "Epoch 20/24\n",
            "----------\n",
            "0 Loss: 0.0114 Acc: 1.0000\n",
            "1 Loss: 0.8244 Acc: 0.6190\n",
            "\n",
            "Epoch 21/24\n",
            "----------\n",
            "0 Loss: 0.0168 Acc: 1.0000\n",
            "1 Loss: 0.8323 Acc: 0.6667\n",
            "\n",
            "Epoch 22/24\n",
            "----------\n",
            "0 Loss: 0.0201 Acc: 1.0000\n",
            "1 Loss: 0.8376 Acc: 0.6667\n",
            "\n",
            "Epoch 23/24\n",
            "----------\n",
            "0 Loss: 0.0099 Acc: 1.0000\n",
            "1 Loss: 0.8344 Acc: 0.6667\n",
            "\n",
            "Epoch 24/24\n",
            "----------\n",
            "0 Loss: 0.0145 Acc: 1.0000\n",
            "1 Loss: 0.8343 Acc: 0.6190\n",
            "\n",
            "Training complete in 9m 59s\n",
            "Best val Acc: 0.714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHeD1yYTjePM"
      },
      "source": [
        "#model_ft = train_model(r3d_18_pre, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\n",
        "#0 is training set\n",
        "#1 is validation set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN9l_6oTL8zH"
      },
      "source": [
        "#Saving the model:\n",
        "import pickle\n",
        "pickle.dump(model_ft, open(\"./modelpa4q1.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nMeyuU4pmfl"
      },
      "source": [
        "import pickle\n",
        "pickle.dump(model_ft, open(\"./modelpa4q1_gpu.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAPtFokTNCzm",
        "outputId": "639fd7f9-2e96-4c7b-ebf4-e5c76ff2681b"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSE527_PA4_fall_21.ipynb  jpl_interaction_segmented_iyuv.zip  modelpa4q1.pkl\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/                     modelpa4q1_gpu.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fnoea8rUfTY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALojw_R5LrRt"
      },
      "source": [
        "## Problem 2: Video Classification\n",
        "### Previous Implementation\n",
        "This dataset was released as a part of the [paper](http://michaelryoo.com/papers/cvpr2013_ryoo.pdf) in CVPR 2013. The paper investigates multichannel kernels to integrate global and local motion information, and presents a new activity learning/recognition methodology that explicitly considers temporal structures displayed in first-person activity videos. As stated in the paper, *We first introduce video features designed to capture\n",
        "global motion (Subsection 3.1) and local motion (Subsection 3.2) observed during humans’ various interactions with\n",
        "the observer. Next, in Subsection 3.3, we cluster features to\n",
        "form visual words and obtain histogram representations. In\n",
        "Subsection 3.4, multi-channel kernels are described.* These features were prepared for an input to the SVM.\n",
        "\n",
        "###Using CNNs\n",
        "In this approach of video classification we are using an image classifier on every single frame of the video. We then have to merge the feature vectors obtained per frames using a fusion layer. This need to be built into the network itself. A Fusion layer is used to merge the output of separate networks that operate on temporally distant frames. It is normally implemented using the max pooling, average pooling or flattening technique. We then define a fully connected layer to provide the output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywkjUNVop-Ku"
      },
      "source": [
        "### Problem 2.1\n",
        "### Temporal Pooling: 20 points\n",
        "As suggested in this [paper](https://arxiv.org/abs/1503.08909), we position the temporal pooling layer right before the ﬁrst fully connected layer as illustrated. This layer performs either mean-pooling or max-pooling across all video frames. The structure of the CNN-component is identical single-frame model. This network is able to collect all the spatial features in a given time window. However, the order of the temporal events is lost due to the nature of pooling across frames\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO0RAD6F_G6T",
        "outputId": "9c6fa6db-b6c3-456e-b9ee-e5f3d6086ee7"
      },
      "source": [
        "import torch\n",
        "pretrained = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxCKEmWUqz1h",
        "outputId": "7863cab6-02ab-443d-ec7a-98d8ac5a82c7"
      },
      "source": [
        "pretrained"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJKuwYnfJ3xk"
      },
      "source": [
        "pretrained.classifier[6] = nn.Linear(in_features=4096, out_features=7, bias=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr_ZLzXbJsR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee09b49-27f3-4077-90f1-2b00d737e7fc"
      },
      "source": [
        "dataset_sizes = []\n",
        "dataset_sizes.append(len(train_ids))\n",
        "dataset_sizes.append(len(test_ids))\n",
        "print(dataset_sizes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[63, 21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkYAcV1e_xRA"
      },
      "source": [
        "#alexnet_pretrained.features[0] = nn.Conv2d(48, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4BpB5rm3SJG"
      },
      "source": [
        "# #train the alexnet\n",
        "# num_epochs = 20\n",
        "# num_classes = 7\n",
        "# batch_size = 8\n",
        "learning_rate = 0.001\n",
        "# # convert all the layers to list and remove the last one\n",
        "# nu_ftrs = alexnet_pretrained.classifier[6].in_features\n",
        "# #print(nu_ftrs)\n",
        "\n",
        "# features = list(alexnet_pretrained.classifier.children())[:-1]\n",
        "# #print(features)\n",
        "# #Updated AlexNet\n",
        "# features.extend([nn.Linear(nu_ftrs,num_classes)])\n",
        "# alexnet_pretrained.classifier = nn.Sequential(*features)\n",
        "# print(alexnet_pretrained.classifier)\n",
        "\n",
        "#Defining the Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(pretrained.parameters(), lr=learning_rate)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D4Knpddc7pb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GRjcWIA3SLz"
      },
      "source": [
        "#--------------------------------------------------\n",
        "#       Train your model\n",
        "#--------------------------------------------------\n",
        "\n",
        "def train_alexnet_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "\n",
        "    since = time.time()\n",
        "    model.to(device)\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in range(len(dataloaders)):\n",
        "            if phase == 0:\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.repeat_interleave(n_frames)\n",
        "                labels = labels.to(device)\n",
        "                inputs = torch.reshape(inputs, ( inputs.size()[0]*inputs.size()[1], inputs.size()[2], inputs.size()[3], inputs.size()[4]) )\n",
        "                #print(inputs.shape)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 0):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    #oss.requires_grad = True\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 0:\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 0:\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / (dataset_sizes[phase]*n_frames)\n",
        "            epoch_acc = running_corrects.double() / (dataset_sizes[phase]*n_frames)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 1 and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    #print()\n",
        "\n",
        "  \n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYcCOE_VHqNv",
        "outputId": "e3d1ee09-a680-4c6f-9658-d1b15928c2fa"
      },
      "source": [
        "dataset_sizes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[63, 21]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4nKgi6euZXL",
        "outputId": "c34eacbc-ee2f-4151-aa28-fe8b2c6af108"
      },
      "source": [
        "ct = 0\n",
        "\n",
        "for child in pretrained.children():\n",
        "  ct = ct+1\n",
        "  if ct <= 1:\n",
        "    print(ct)\n",
        "    print(child)\n",
        "    for param in child.parameters():\n",
        "      param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (7): ReLU(inplace=True)\n",
            "  (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (9): ReLU(inplace=True)\n",
            "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t2VnmxaF0A8",
        "outputId": "f6da58c4-3a5c-4c51-e0e9-f82dd2185d1a"
      },
      "source": [
        "pre_trained2 = train_alexnet_model(pretrained, criterion, optimizer, exp_lr_scheduler, num_epochs=25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "0 Loss: 2.1141 Acc: 0.1608\n",
            "1 Loss: 1.8102 Acc: 0.2873\n",
            "Epoch 1/24\n",
            "----------\n",
            "0 Loss: 1.6934 Acc: 0.3376\n",
            "1 Loss: 1.6809 Acc: 0.3413\n",
            "Epoch 2/24\n",
            "----------\n",
            "0 Loss: 1.5672 Acc: 0.3942\n",
            "1 Loss: 1.6179 Acc: 0.4127\n",
            "Epoch 3/24\n",
            "----------\n",
            "0 Loss: 1.3974 Acc: 0.4884\n",
            "1 Loss: 1.5720 Acc: 0.4159\n",
            "Epoch 4/24\n",
            "----------\n",
            "0 Loss: 1.3143 Acc: 0.4942\n",
            "1 Loss: 1.5159 Acc: 0.4937\n",
            "Epoch 5/24\n",
            "----------\n",
            "0 Loss: 1.2745 Acc: 0.5228\n",
            "1 Loss: 1.5036 Acc: 0.4317\n",
            "Epoch 6/24\n",
            "----------\n",
            "0 Loss: 1.1885 Acc: 0.5677\n",
            "1 Loss: 1.4547 Acc: 0.4873\n",
            "Epoch 7/24\n",
            "----------\n",
            "0 Loss: 1.0925 Acc: 0.6233\n",
            "1 Loss: 1.4485 Acc: 0.5048\n",
            "Epoch 8/24\n",
            "----------\n",
            "0 Loss: 1.0613 Acc: 0.6471\n",
            "1 Loss: 1.4443 Acc: 0.5206\n",
            "Epoch 9/24\n",
            "----------\n",
            "0 Loss: 1.0689 Acc: 0.6471\n",
            "1 Loss: 1.4429 Acc: 0.5095\n",
            "Epoch 10/24\n",
            "----------\n",
            "0 Loss: 1.0654 Acc: 0.6497\n",
            "1 Loss: 1.4416 Acc: 0.5127\n",
            "Epoch 11/24\n",
            "----------\n",
            "0 Loss: 1.0404 Acc: 0.6741\n",
            "1 Loss: 1.4390 Acc: 0.5048\n",
            "Epoch 12/24\n",
            "----------\n",
            "0 Loss: 1.0314 Acc: 0.6772\n",
            "1 Loss: 1.4365 Acc: 0.5048\n",
            "Epoch 13/24\n",
            "----------\n",
            "0 Loss: 1.0379 Acc: 0.6762\n",
            "1 Loss: 1.4351 Acc: 0.5032\n",
            "Epoch 14/24\n",
            "----------\n",
            "0 Loss: 1.0205 Acc: 0.6857\n",
            "1 Loss: 1.4348 Acc: 0.5032\n",
            "Epoch 15/24\n",
            "----------\n",
            "0 Loss: 1.0223 Acc: 0.6608\n",
            "1 Loss: 1.4347 Acc: 0.5032\n",
            "Epoch 16/24\n",
            "----------\n",
            "0 Loss: 1.0239 Acc: 0.6868\n",
            "1 Loss: 1.4344 Acc: 0.5032\n",
            "Epoch 17/24\n",
            "----------\n",
            "0 Loss: 1.0288 Acc: 0.6704\n",
            "1 Loss: 1.4342 Acc: 0.5032\n",
            "Epoch 18/24\n",
            "----------\n",
            "0 Loss: 1.0236 Acc: 0.6804\n",
            "1 Loss: 1.4340 Acc: 0.5032\n",
            "Epoch 19/24\n",
            "----------\n",
            "0 Loss: 1.0281 Acc: 0.6772\n",
            "1 Loss: 1.4338 Acc: 0.5032\n",
            "Epoch 20/24\n",
            "----------\n",
            "0 Loss: 1.0199 Acc: 0.6762\n",
            "1 Loss: 1.4335 Acc: 0.5048\n",
            "Epoch 21/24\n",
            "----------\n",
            "0 Loss: 1.0170 Acc: 0.6847\n",
            "1 Loss: 1.4335 Acc: 0.5048\n",
            "Epoch 22/24\n",
            "----------\n",
            "0 Loss: 1.0229 Acc: 0.6788\n",
            "1 Loss: 1.4335 Acc: 0.5048\n",
            "Epoch 23/24\n",
            "----------\n",
            "0 Loss: 1.0081 Acc: 0.6899\n",
            "1 Loss: 1.4334 Acc: 0.5048\n",
            "Epoch 24/24\n",
            "----------\n",
            "0 Loss: 1.0206 Acc: 0.6772\n",
            "1 Loss: 1.4334 Acc: 0.5048\n",
            "Training complete in 7m 15s\n",
            "Best val Acc: 0.520635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "ac83f6e8711e4b668fcf8c9a7dfbdef6",
            "26115af9c3c641fa82fcf09dac4a7047",
            "cd090eb18ca54aee9555a740ee9019a1",
            "8107926ed73d430eb4ea20a8d2d580ed",
            "43f7475bacb84546b8733dd453bac31c",
            "8c697d47e86a4301a2f4ac95a9c6a93d",
            "29d3aa422aec47318ecfbc097f813222",
            "7cf49bf4e7f9439da1f56f3e17fa627b",
            "d14ba2a7597e4474b99d44326dcfd02a",
            "263b47f572a2435584d7896413b77a3b",
            "8c9cf839b5834178b2e129d6573d48d6"
          ]
        },
        "id": "UkkoqnXpkVjZ",
        "outputId": "be3915d1-6ecd-4966-e103-18ff3dea434f"
      },
      "source": [
        "import torch\n",
        "resnet = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac83f6e8711e4b668fcf8c9a7dfbdef6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/49.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfY91ti0uEkD",
        "outputId": "bd6ec3c3-143c-4936-af19-f210e1c75ffb"
      },
      "source": [
        "resnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogLeNet(\n",
              "  (conv1): BasicConv2d(\n",
              "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  (conv2): BasicConv2d(\n",
              "    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv3): BasicConv2d(\n",
              "    (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  (inception3a): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (inception3b): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  (inception4a): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(208, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (inception4b): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (inception4c): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (inception4d): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (inception4e): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  (inception5a): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (inception5b): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (aux1): None\n",
              "  (aux2): None\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwjvIP-_vrxF"
      },
      "source": [
        "resnet.fc =  nn.Linear(in_features=1024, out_features=7, bias=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjyzHEXBuN-a",
        "outputId": "3dcf3139-e7fb-4a1a-efe5-4f5e20ba6586"
      },
      "source": [
        "ct = 0\n",
        "\n",
        "for child in resnet.children():\n",
        "    ct = ct+1\n",
        "    if ct <= 6:\n",
        "      print(ct, \"------------------\")\n",
        "      print(child)\n",
        "      for param in child.parameters():\n",
        "        param.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 ------------------\n",
            "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "2 ------------------\n",
            "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "3 ------------------\n",
            "ReLU(inplace=True)\n",
            "4 ------------------\n",
            "MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "5 ------------------\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "6 ------------------\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhHuoUPRPgfp"
      },
      "source": [
        "#Saving the model:\n",
        "import pickle\n",
        "pickle.dump(pre_trained2, open(\"./alexnet_trained_nov30_130_v2.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z31eOytaPV4G",
        "outputId": "c7980f02-f661-4ba3-d21f-331aa7c06c0a"
      },
      "source": [
        "def removeLayer(alex_net):    \n",
        "    new_classifier = nn.Sequential(*list(alex_net.classifier.children())[:-1])\n",
        "    alex_net.classifier = new_classifier\n",
        "    return alex_net\n",
        "#alex_net = models.alexnet(pretrained=True)\n",
        "#print(alex_net)\n",
        "alex_net = removeLayer(pre_trained2)\n",
        "print(alex_net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIDxZ36unAJi"
      },
      "source": [
        "alex_net=pre_trained2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FanTSgg6oWXp",
        "outputId": "3a80f078-adb8-456d-da92-26033b63124b"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "834"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzZFIMVjSdB5",
        "outputId": "2f06abbc-e2f1-4bab-9b45-5b9356bbe5ab"
      },
      "source": [
        "alex_net"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF3OMQk2Q0eY"
      },
      "source": [
        "#Extract feature:\n",
        "def extract_imagenet_feature(dl):\n",
        "\n",
        "  feature = []\n",
        "  \n",
        "  for input, labels in dl:\n",
        "    for i in range(len(input)):\n",
        "      inputs = input[i]\n",
        "      inputs.to(device)\n",
        "      #print(inputs.shape)\n",
        "      inputs = torch.reshape(inputs, ( inputs.size()[0], inputs.size()[1], inputs.size()[2], inputs.size()[3]) )\n",
        "      #alex_net.to(device)\n",
        "      output = alex_net(inputs.to(device))\n",
        "      feature.append(output)\n",
        "  \n",
        "  feature_tensor = torch.stack(feature)\n",
        "\n",
        "  return feature_tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRbNCSw6qQI3",
        "outputId": "95dab3da-32fd-420b-fb93-21e5e2259840"
      },
      "source": [
        "train_feauture = extract_imagenet_feature(dataloaders[0])\n",
        "test_feauture = extract_imagenet_feature(dataloaders[1])\n",
        "print(len(train_feauture), train_feauture.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63 torch.Size([63, 30, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNGIDkAWnX0d"
      },
      "source": [
        "#--------------------------------------------------\n",
        "#       Utilities for Temporal pooling\n",
        "#--------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnNweQdXaddu"
      },
      "source": [
        "from torch.nn import MaxPool3d, AvgPool3d\n",
        "from torch.nn import MaxPool2d, AvgPool2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4To2Qvg4UOiV",
        "outputId": "75f5cf44-a3ca-4130-96de-0949d45084b1"
      },
      "source": [
        "pool = AvgPool2d(30,1)\n",
        "#train_feature_tensor = torch.stack(train_feature)\n",
        "train_feature_pool = pool(train_feauture)\n",
        "test_feature_pool = pool(test_feauture)\n",
        "\n",
        "print(train_feature_pool.shape, test_feature_pool.shape)\n",
        "\n",
        "#Convert roch to Numpy\n",
        "test_feature_np = test_feature_pool.cpu().detach().numpy()\n",
        "train_feature_np = train_feature_pool.cpu().detach().numpy()\n",
        "\n",
        "train_feature_np = train_feature_np[:,0,:]\n",
        "test_feature_np = test_feature_np[:,0,:]\n",
        "\n",
        "print(train_feature_np.shape, test_feature_np.shape)\n",
        "\n",
        "#Extract labels\n",
        "test_labels_np = np.array(test_labels)\n",
        "train_labels_np = np.array(train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([63, 1, 4067]) torch.Size([21, 1, 4067])\n",
            "(63, 4067) (21, 4067)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFW6V4NUI3VX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3WwKr0vgybv"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPXyx98wjKhx",
        "outputId": "9fc3a196-3477-4bda-c3d3-6dba47ba3731"
      },
      "source": [
        "print(train_labels_np.shape, train_feature_np.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(63,) (63, 4067)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7aYexJRJypE",
        "outputId": "4b28a0ba-7e5e-49e8-8606-ff20a7642357"
      },
      "source": [
        "train_labels_np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 4, 5, 6, 0, 1,\n",
              "       2, 3, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 6, 0, 1, 2,\n",
              "       3, 4, 6, 0, 1, 2, 4, 6, 5, 5, 5, 3, 6, 5, 3, 0, 2, 4, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frsZj6_hM-SJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f44d581b-7f0b-4b34-9ce4-7e5705c2ebfb"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# split data into train and test sets\n",
        "\n",
        "def modelfit(t):\n",
        "  \n",
        "  #clf = KNeighborsClassifier(n_neighbors=10, algorithm=\"kd_tree\")\n",
        "  #clf = DecisionTreeClassifier(max_depth=8, random_state=t)\n",
        "  clf = svm.SVC(kernel = \"poly\", max_iter=50000, degree=2, tol = 1e-5,coef0=0,shrinking=True,decision_function_shape='ovr',gamma='scale')\n",
        "  #clf = XGBClassifier( learning_rate = 0.01,max_depth = 5 )\n",
        "\n",
        "  clf.fit(train_feature_np, train_labels_np)\n",
        "\n",
        "  def calulate(dataset, labels, data):\n",
        "\n",
        "    prediction = [-1 for _ in range(len(dataset))]\n",
        "    for i in range(len(dataset)):\n",
        "      prediction[i] = clf.predict(np.reshape(dataset[i],(1,-1)))\n",
        "\n",
        "    label_pred = np.reshape(np.array(prediction),(-1,))\n",
        "    #print(label_pred)\n",
        "    #The prediction is test_label_pred\n",
        "    accuracy = sum(np.array(labels) == label_pred) / float(len(labels))\n",
        "    #list.append(accuracy)\n",
        "    print(\"The accuracy of model is\", data,  \"{:.2f}%\".format(accuracy*100))\n",
        "\n",
        "  calulate(train_feature_np, train_labels_np, \"Train\")\n",
        "  calulate(test_feature_np, test_labels_np, \"Test\")\n",
        "\n",
        "modelfit(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of model is Train 90.48%\n",
            "The accuracy of model is Test 38.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlcLiUeUnVmM"
      },
      "source": [
        "### Problem 2.2\n",
        "### Network Definition: 20 points\n",
        "### Feature Extraction using an ImageNet pre-trained CNN \n",
        "Use a fine-tuned resNet model that you used in Part1 to extract the features from every video frames.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLG3WtEmYYM3"
      },
      "source": [
        "#--------------------------------------------------\n",
        "#Define your  Vid_Classifier\n",
        "#you may add extra parameters here.\n",
        "#remember to define :\n",
        "  # a base model which is an ImageNet pre-trained CNN : Extract One feature vector per frame\n",
        "  # a max pooling layer that finds the maximum feature map over a local temporal neighborhood\n",
        "  # a fully connected layer to unify the feature maps\n",
        "#You may also want to include other parameters for your module\n",
        "#if you are using a knn classifer, please indicate it well with your code.\n",
        "#--------------------------------------------------\n",
        "  \n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "class Vid_Classifier(nn.Module):\n",
        "    def __init__(self, params_model):\n",
        "        super(Vid_Classifier, self).__init__()\n",
        "        num_classes = params_model[\"num_classes\"]\n",
        "        dr_rate= params_model[\"dr_rate\"] #drop out rate\n",
        "        pretrained = params_model[\"pretrained\"]\n",
        "        #--------------------------------------------------\n",
        "        #Your code here\n",
        "        #--------------------------------------------------\n",
        "             \n",
        "    def forward(self, x):\n",
        "        #--------------------------------------------------\n",
        "        #Your code here\n",
        "        #--------------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1r7_tWN4jLb"
      },
      "source": [
        "num_classes = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btOal_ampEnm"
      },
      "source": [
        "params_model={\n",
        "        \"num_classes\": num_classes,\n",
        "        \"dr_rate\": 0.1,\n",
        "        \"pretrained\" : True,}\n",
        "model = Vid_Classifier(params_model) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojzdYPKP4UQN"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i69EIC7B5EUc"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrscW69XWmSt"
      },
      "source": [
        "# path2weights = \"./models/weights.pt\"\n",
        "# torch.save(model.state_dict(), path2weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzEaPw-kMudB"
      },
      "source": [
        "### Problem 2.2\n",
        "**Train and Test:10 points**\n",
        "### Problem 2.2\n",
        "**Fusion based implementation: 20 bonus points**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1Ng2cjpXhT2"
      },
      "source": [
        "#define your training function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLYjz3jSMg9_"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRw9JWtMSrWi"
      },
      "source": [
        "**Answer**:\n",
        "\n",
        "Accuracy on test set: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UGefIYpYYNF"
      },
      "source": [
        "## Submission guidelines\n",
        "---\n",
        "Your need to submit a single zip file to Blackboard described as follow.\n",
        "\n",
        "Please generate a pdf file that includes a ***google shared link*** (explained in the next paragraph). This pdf file should be named as ***Surname_Givenname_SBUID_pa*\\*.pdf** (example: Jordan_Michael_111234567_pa3.pdf for this assignment).\n",
        "\n",
        "To generate the ***google shared link***, first create a folder named ***Surname_Givenname_SBUID_pa**** in your Google Drive with your Stony Brook account. The structure of the files in the folder should be exactly the same as the one you downloaded. For instance in this homework:\n",
        "\n",
        "```\n",
        "Surname_Givenname_SBUID_pa4\n",
        "        |---data\n",
        "        |---CSE527-PA4-fall21.ipynb\n",
        "```\n",
        "Note that this folder should be in your Google Drive with your Stony Brook account.\n",
        "\n",
        "Then right click this folder, click ***Get shareable link***, in the People textfield, enter the TA's email: ***bjha@cs.stonybrook.edu***, ***li.wenchen@stonybrook.edu***, ***yifeng.huang@stonybrook.edu***. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **UNCHECK** the **Notify people** box.\n",
        "\n",
        "Note that in google colab, we will only grade the version of the code right before the timestamp of the submission made in blackboard. \n",
        "\n",
        "To submit to Blackboard, zip ***Surname_Givenname_SBUID_pa*\\*.pdf** and ***Surname_Givenname_SBUID_pa**** folder together and name your zip file as ***Surname_Givenname_SBUID_pa*\\*.zip**. \n",
        "\n",
        "**DO NOT upload the datasets to Blackboard.**\n",
        "\n",
        "The input and output paths are predefined and **DO NOT** change them, (we assume that 'Surname_Givenname_SBUID_pa4' is your working directory, and all the paths are relative to this directory).  The image read and write functions are already written for you. All you need to do is to fill in the blanks as indicated to generate proper outputs.\n",
        "\n",
        "\n",
        "-- DO NOT change the folder structure, please just fill in the blanks. <br>\n",
        "\n",
        "You are encouraged to post and answer questions on Piazza. Based on the amount of email that we have received in past years, there might be dealys in replying to personal emails. Please ask questions on Piazza and send emails only for personal issues.\n",
        "\n",
        "If you alter the folder structures, the grading of your homework will be significantly delayed and possibly penalized.\n",
        "\n",
        "Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYJOi8QYYYNG"
      },
      "source": [
        "<!--Write your report here in markdown or html-->\n"
      ]
    }
  ]
}